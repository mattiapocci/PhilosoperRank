{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapingWikiDumps.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gI8Q628Jwhu_",
        "GwUcm0KqR2c-",
        "w1QRTdVPwskg",
        "kU4Kft8-GKvT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiapocci/PhilosopherRank/blob/master/scrapingWikiDumps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI8Q628Jwhu_",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzk8gP9t-sU3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc1f24ef-faae-4881-b51d-a3c59535d532"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwUcm0KqR2c-",
        "colab_type": "text"
      },
      "source": [
        "## Download and extract Wikipedia dump\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nwKVtmI_YQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget  -P \"/content/drive/My Drive/Wiki_dump/\" \"https://dumps.wikimedia.org/enwiki/20200120/enwiki-20200120-pages-articles-multistream.xml.bz2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Ul1mbbukNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bunzip2 -d -k -s /content/drive/My\\ Drive/Wiki_dump/enwiki-20200120-pages-articles-multistream.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSeuNCsgY64U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 WikiExtractor.py \"enwiki-20200120-pages-articles-multistream.xml\"  --json --processes 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1QRTdVPwskg",
        "colab_type": "text"
      },
      "source": [
        "# Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU4Kft8-GKvT",
        "colab_type": "text"
      },
      "source": [
        "## Parsing utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5KljeSoo7M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_valid_json(filename):\n",
        "    \"\"\"\n",
        "    Create a valid json with the commas and the square brackets\n",
        "    :param string:\n",
        "    :return: None \n",
        "    \"\"\"\n",
        "    with open(filename, 'r+') as f:\n",
        "        data = f.read().replace('}', '},')\n",
        "        data = data[:-2]\n",
        "        f.seek(0, 0)\n",
        "        f.write('['.rstrip('\\r\\n') + '\\n' + data)\n",
        "        \n",
        "    with open(filename, 'a') as f:\n",
        "        f.write(\"]\")\n",
        "\n",
        "\n",
        "def find_matches(filename,word,phil_list):\n",
        "    \"\"\"\n",
        "    Find matches inside the articles with the given word and add it to the given phil_list\n",
        "    :param string:\n",
        "    :param string:\n",
        "    :param list:\n",
        "    :return: list of the articles containg the word philosopher\n",
        "    \"\"\"\n",
        "    #phil_list = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        try:\n",
        "            data = json.loads(file.read())\n",
        "            for article in data:\n",
        "                if word in article['text']:\n",
        "                    phil_list.append(article)\n",
        "            return phil_list\n",
        "        except:\n",
        "            print(\"Error with: \", filename)\n",
        "\n",
        "\n",
        "def write_json(data, name):\n",
        "    \"\"\"\n",
        "    Write into a file the data given with the name given\n",
        "    :param lst of json:\n",
        "    :param string:\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    with open('/content/drive/My Drive/Wiki_dump/'+name, 'w') as outfile:\n",
        "        json.dump(data, outfile)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZkURNTLFho4",
        "colab_type": "text"
      },
      "source": [
        "## Executing the parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ene_FPbdKO2f",
        "colab_type": "text"
      },
      "source": [
        "If the wikipedia articles are yet parsed with the create_valid_json function, do NOT redo the parsing, otherwise you will mess all the articles!!! To avoid this, comment the indicated line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gy9_qi6w25y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_data(rootdir=\"/home/luigi/Downloads/wir/wikiextractor-master/text/\"):\n",
        "    \"\"\"\n",
        "    Parse all the files presents in the rootdir and its subdirectories\n",
        "    :param string:\n",
        "    :return: None \n",
        "    \"\"\"\n",
        "    lst = []\n",
        "    for subdir, dirs, files in os.walk(rootdir):\n",
        "        for file in files:\n",
        "            filename_fullpath = os.path.join(subdir, file)\n",
        "            create_valid_json(filename_fullpath) #THIS LINE TO BE COMMENTED according to what is written above\n",
        "            lst = find_matches(filename_fullpath,\"philosopher\",lst)\n",
        "    write_json(lst,file+\".json\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY0JvqLuHGuW",
        "colab_type": "text"
      },
      "source": [
        "Formatting the data to be uniform with the beautiful soup data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egd0XqaOHFK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lst = []\n",
        "with open(\"/content/drive/My Drive/Wiki_dump/wiki_75.json\", 'r', encoding='utf-8') as file:\n",
        "    data = json.loads(file.read())\n",
        "    for article in data:\n",
        "        if \"philosopher\" in article['text']:\n",
        "            var = prototype = {\n",
        "                                \"philosopher\":\"\" ,\n",
        "                                \"article\": \"\",\n",
        "                                \"pageid\": \"\",\n",
        "                                \"table_influenced\": [],\n",
        "                                \"table_influences\": []\n",
        "                                }\n",
        "            var['philosopher'] = article['title']\n",
        "            var['article'] = article['text']\n",
        "            var['pageid'] = article['id']\n",
        "            lst.append(var)\n",
        "    write_json(lst,\"uniformat.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEPbpBZxRI7D",
        "colab_type": "text"
      },
      "source": [
        "# Compare with the category dump"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3RYE6IapF-V",
        "colab_type": "text"
      },
      "source": [
        "Construct lists from the entire dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPdC4r1jSHue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_a_phil_dump = []\n",
        "born_lived_dump = []\n",
        "\n",
        "with open(\"/content/drive/My Drive/Wiki_dump/uniformat.json\", 'r', encoding='utf-8') as file:\n",
        "    data = json.loads(file.read())\n",
        "    for article in data:\n",
        "        if \"born\" in article['article'] or \"lived\" in article['article']:\n",
        "            born_lived_dump.append(article)\n",
        "        if re.match(r\".*a.*philosopher\",article['article']):\n",
        "            reg_a_phil_dump.append(article)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgGBUbD0pLp7",
        "colab_type": "text"
      },
      "source": [
        "construct lists from the category dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX0jATXKbhFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "born_lived = []\n",
        "phil = []\n",
        "reg_a_phil = []\n",
        "with open(\"/content/drive/My Drive/Wiki_dump/mattia_ground_t.json\", 'r', encoding='utf-8') as file:\n",
        "    data_cat = json.loads(file.read())\n",
        "for a in data_cat:\n",
        "    if 'philosopher' in a['article']:\n",
        "        phil.append(a)\n",
        "        if \"born\" in a['article'] or \"lived\" in a['article']:\n",
        "            born_lived.append(a)\n",
        "        if re.match(r\".*a.*philosopher\",a['article']):\n",
        "            reg_a_phil.append(a)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xes5yX_pgqD",
        "colab_type": "text"
      },
      "source": [
        "Finding the articles presents in both the dumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz5csTzApg-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "match = 0\n",
        "trovato = False\n",
        "for cat_art in phil:\n",
        "    trovato = False\n",
        "    for dump_art in data:\n",
        "        if not trovato and cat_art['pageid'] == dump_art['pageid']:\n",
        "            match = match + 1\n",
        "            trovato = True\n",
        "    if not trovato:\n",
        "        print(cat_art)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmHSamN6pRiz",
        "colab_type": "text"
      },
      "source": [
        "Printing the scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-6HA4l0pRsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7055dba5-5f20-469a-cb3b-38a4d54f980f"
      },
      "source": [
        "print(\"===============DATA ANALYSIS OF CATEGORY DUMP ============\")\n",
        "print(\"Length of category dump: \",len(data_cat))\n",
        "print(\"Length with 'philosopher' in article: \",len(phil))\n",
        "print(\"Length with 'born' or 'lived' in article: \",len(born_lived))\n",
        "print(\"Length with regex '.* a.* philosopher' in article: \",len(reg_a_phil))\n",
        "print(\"\\n\")\n",
        "print(\"===============DATA ANALYSIS OF ENTIRE DUMP ============\")\n",
        "print(\"Length of dump: 62000000\")\n",
        "print(\"Length with 'philosopher' in article: \",len(data))\n",
        "print(\"Length with 'born' or 'lived' in article: \",len(born_lived_dump))\n",
        "print(\"Length with regex '.* a.* philosopher' in article: \",len(reg_a_phil_dump))\n",
        "print(\"\\n\")\n",
        "print(\"Matched \",match,\" articles between category dump and all wiki\")\n",
        "print(\"Missing\",len(phil)-match,\" articles from all dump\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============DATA ANALYSIS OF CATEGORY DUMP ============\n",
            "Length of category dump:  1712\n",
            "Length with 'philosopher' in article:  1161\n",
            "Length with 'born' or 'lived' in article:  968\n",
            "Length with regex '.* a.* philosopher' in article:  996\n",
            "\n",
            "\n",
            "===============DATA ANALYSIS OF ENTIRE DUMP ============\n",
            "Length of dump: 62000000\n",
            "Length with 'philosopher' in article:  26312\n",
            "Length with 'born' or 'lived' in article:  14272\n",
            "Length with regex '.* a.* philosopher' in article:  309\n",
            "\n",
            "\n",
            "Matched  1131  articles between category dump and all wiki\n",
            "Missing 30  articles from all dump\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}