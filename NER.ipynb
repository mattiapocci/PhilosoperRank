{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiapocci/PhilosoperRank/blob/master/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0EkabEj_ah5",
        "colab_type": "text"
      },
      "source": [
        "#***Imports and Drive Mount***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcg7xL1K7yyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_test_labels(_data_gen_test, _data_out, classification_mode, quick_test):\n",
        "    # Collecting ground truth for test data\n",
        "    nb_batch = 2 if quick_test else _data_gen_test.get_total_batches_in_data()\n",
        "\n",
        "    batch_size = _data_out[0][0]\n",
        "    gt_sed = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[0][2]))\n",
        "    gt_doa = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[1][2]))\n",
        "\n",
        "    print(\"nb_batch in test: {}\".format(nb_batch))\n",
        "    cnt = 0\n",
        "    for tmp_feat, tmp_label in _data_gen_test.generate():\n",
        "        gt_sed[cnt * batch_size:(cnt + 1) * batch_size, :, :] = tmp_label[0]\n",
        "        gt_doa[cnt * batch_size:(cnt + 1) * batch_size, :, :] = tmp_label[1]\n",
        "        cnt = cnt + 1\n",
        "        if cnt == nb_batch:\n",
        "            break\n",
        "    return gt_sed.astype(int), gt_doa\n",
        "    \n",
        "def print_generic_evaluation(model, _dataset, _ov, _split):\n",
        "    data_gen_test = DataGenerator(\n",
        "            dataset=_dataset, ov=_ov, split=_split, db=params['db'], nfft=params['nfft'],\n",
        "            batch_size=params['batch_size'], seq_len=params['sequence_length'], classifier_mode=params['mode'],\n",
        "            weakness=params['weakness'], datagen_mode='test', cnn3d=params['cnn_3d'], xyz_def_zero=params['xyz_def_zero'],\n",
        "            azi_only=params['azi_only'], shuffle=False\n",
        "        )\n",
        "    \n",
        "    values = model.evaluate_generator(data_gen_test.generate(),\n",
        "                                steps=2 if params['quick_test'] else data_gen_test.get_total_batches_in_data(),\n",
        "                                verbose = 1)\n",
        "    print(\"Accuracy and Loss evaluation for dataset\", _dataset,\n",
        "          \"ov\", _ov, \"split\", _split)\n",
        "    i = 0\n",
        "    for name in model.metrics_names:\n",
        "        print(name, values[i])\n",
        "        i+=1\n",
        "\n",
        "def print_sed_doa_scores(model, _dataset, _ov, _split):\n",
        "    data_gen_test = DataGenerator(\n",
        "            dataset=_dataset, ov=_ov, split=_split, db=params['db'], nfft=params['nfft'],\n",
        "            batch_size=params['batch_size'], seq_len=params['sequence_length'], classifier_mode=params['mode'],\n",
        "            weakness=params['weakness'], datagen_mode='test', cnn3d=params['cnn_3d'], xyz_def_zero=params['xyz_def_zero'],\n",
        "            azi_only=params['azi_only'], shuffle=False\n",
        "        )\n",
        "\n",
        "    gt = collect_test_labels(data_gen_test, data_out, params['mode'], params['quick_test'])\n",
        "    sed_gt = reshape_3Dto2D(gt[0])\n",
        "    doa_gt = reshape_3Dto2D(gt[1])\n",
        "\n",
        "    pred = model.predict_generator(\n",
        "                generator=data_gen_test_1.generate(),\n",
        "                steps=2 if params['quick_test'] else data_gen_test.get_total_batches_in_data(),\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "    sed_pred = reshape_3Dto2D(pred[0]) > 0.5\n",
        "    doa_pred = reshape_3Dto2D(pred[1])\n",
        "\n",
        "    sed_scores = compute_sed_scores(sed_pred, sed_gt, data_gen_test.nb_frames_1s())\n",
        "    if params['azi_only']:\n",
        "        doa_scores, conf_mat = compute_doa_scores_regr_xy(doa_pred, doa_gt, sed_pred, sed_gt)\n",
        "    else:\n",
        "        doa_scores, conf_mat = compute_doa_scores_regr_xyz(doa_pred, doa_gt, sed_pred, sed_gt)\n",
        "        \n",
        "    #print('Confusion Mx : {}'.format(conf_mat))\n",
        "    #print('best_conf_mat_diag : {}'.format(np.diag(best_conf_mat)))\n",
        "    print('SED Metrics: ER_overall: {}, F1_overall: {}'.format(sed_scores[0], sed_scores[1]))\n",
        "    print('DOA Metrics: doa_loss_gt: {}, doa_loss_pred: {}, good_pks_ratio: {}'.format(\n",
        "        doa_scores[1], doa_scores[2], doa_scores[5] / float(sed_gt.shape[0])))\n",
        "    \n",
        "print_generic_evaluation(model, 'ansim', 1, 1)\n",
        "#print_generic_evaluation(model, 'ansim', 1, 2)\n",
        "#print_generic_evaluation(model, 'ansim', 1, 3)\n",
        "print_sed_doa_scores(model, 'ansim', 1, 1)\n",
        "#print_sed_doa_scores(model, 'ansim', 1, 2)\n",
        "#print_sed_doa_scores(model, 'ansim', 1, 3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}