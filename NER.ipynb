{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiapocci/PhilosoperRank/blob/master/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0EkabEj_ah5",
        "colab_type": "text"
      },
      "source": [
        "#***Imports and Drive Mount***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcg7xL1K7yyZ",
        "colab_type": "code",
        "outputId": "5838f5eb-1d7f-42fb-d96d-faca05d6be4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "!pip3 install -U spacy[cuda92]\n",
        "!python3 -m spacy download en_core_web_lg\n",
        "#https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anacon\n",
        "!python3 -m spacy link en_core_web_lg en_lg\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "import pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "\n",
        "nlp = spacy.load(\"en_lg\")\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy[cuda92] in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (46.3.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.2)\n",
            "Collecting cupy-cuda92>=5.0.0b4; extra == \"cuda92\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/70/1a297362b6f52462c174a15ebf9c256717fa9d0115c34e5b102c5c497255/cupy_cuda92-8.0.0b2-cp36-cp36m-manylinux1_x86_64.whl (324.1MB)\n",
            "\u001b[K     |████████████████████████████████| 324.1MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[cuda92]) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda92>=5.0.0b4; extra == \"cuda92\"->spacy[cuda92]) (0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[cuda92]) (3.1.0)\n",
            "Installing collected packages: cupy-cuda92\n",
            "Successfully installed cupy-cuda92-8.0.0b2\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=2a07c06e074c9b80c12db3cfd12e5abbeee92d2786bfce3b6f7a2a9aa913277c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jghiom4k/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en_lg\n",
            "You can now load the model via spacy.load('en_lg')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCF3XNrOyZGk",
        "colab_type": "text"
      },
      "source": [
        "# ***Testing Dataset***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xqGWf8yWr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = {}\n",
        "phrases_influence=[[\"He was an influence of her\",[\"her\"],[\"He\"]],\n",
        "                   [\"Jhon was an influence of Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Milton was the main influence of Mallio and Tullio\",[\"Mallio\",\"Tullio\"],[\"Milton\"]],\n",
        "                   [\"Jhon's influence on Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Markov's influence on Tito is the main reason of \\\"La Bella e la Bestia\\\", the main work of Tito's career. \",[\"Tito\"],[\"Markov\"]],\n",
        "                   [\"Jhon was Mark's influence\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"At the university she was introduced to the writings of Aristotle and Plato, who would be her greatest influence and counter-influence respectively\",[\"she\"],[\"Aristotele\", \"Plato\"]],\n",
        "                   [\"Jhon had influence on Mark's work\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Hegel had an important influence on all the different works of Einstein career\",[\"Einstein\"],[\"Hegel\"]],\n",
        "                   [\"Mark cites Jhon as an influence\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Hegel cited Shopenauer as the main influence of his work\",[\"Hegel\"],[\"Shopenauer\"]],\n",
        "                   [\"Jhon was cited by Mark as an influence\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Adler is cited as an influence on John Milton's testament\",[\"Jhon\", \"Milton\"],[\"Adler\"]]]\n",
        "phrases_influences=[[\"influences include him\",[],[\"him\"]],\n",
        "                   [\"influences include Jhon\",[],[\"Jhon\"]],\n",
        "                   [\"influences of his work include Jhon Milton and the author of  \\\"La Casa\\\", Rick Morty\",[],[\"Jhon\", \"Milton\",\"Rick\",\"Morty\"]],\n",
        "                   [\"Jhon and Adam were Mark's influences\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Aristotele, Anassagora and Pluto were Antichiola main influences\",[\"Antichiola\"],[\"Pluto\",\"Aristotele\",\"Anassagora\"]],\n",
        "                   [\"Jhon and Adam where influences of Mark\",[\"Mark\"],[\"Jhon\",\"Adam\"]],\n",
        "                   [\"Macchiavelli and Dante Alighieri where the main influences of Boccaccio's Decameron\",[\"Boccaccio\"],[\"Macchiavelli\",\"Dante\", \"Alighieri\"]]]\n",
        "phrases_influenced=[[\"He was influenced by him\",[\"He\"],[\"him\"]],\n",
        "                   [\"Mark was influenced by Jhon\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Adler was also influenced by the philosophies of Immanuel Kant, Friedrich Nietzsche, Rudolf Virchow and the statesman Jan Smuts\",[\"Adler\"],[\"Immanuel\", \"Kant\", \"Friedrich\", \"Nietzsche\", \"Rudolf\", \"Virchow\", \"Jan\", \"Smuts\"]],\n",
        "                   [\"Jhon has influenced Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Macchiavelli with his work have deeply influenced the philosophies of Mathma Ghandi\",[\"Mathma\",\"Ghandi\"],[\"Macchiavelli\"]],\n",
        "                   [\"Jhon influenced Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Anassagora influenced Pitagora on the ideas regarding geometry\",[\"Pitagora\"],[\"Anassagora\"]],\n",
        "                   [\"Mark's work was influenced by Jhon\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Gandhi's \\\"Power to the Man\\\" was influenced by the medioeval thinker Mark Robben\",[\"Ghandi\"],[\"Mark\", \"Robben\"]],\n",
        "                   [\"Jhon's work influenced Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Marconi's work on the electromagnetic field influenced Einstein's ideas of photon's transmission\",[\"Einstein\"],[\"Marconi\"]],\n",
        "                   [\"Jhon influenced Mark's work\",[\"Mark\"],[\"Jhon\"]],\n",
        "                   [\"Pluto influenced the idea of time of Aristotele and Minos\",[\"Aristotele\", \"Minos\"],[\"Pluto\"]]]\n",
        "phrases_inspire=[\n",
        "                    [\"He seemed to inspire her\",[\"her\"],[\"He\"]],\n",
        "                    [\"These revolutionizing ideas of Wang Yangming would later inspire prominent Japanese thinkers like Motoori Norinaga\",[\"Mootori Norinaga\"],[\"Wang Yangming\"]],\n",
        "                    [\"John's work inspire Mark\",[\"John\"],[\"Mark\"]]\n",
        "                ]\n",
        "phrases_inspiration=[\n",
        "                        [\"John was inspiration for Mark\",[\"Mark\"],[\"John\"]],\n",
        "                        [\"Rozanov is the main source of inspiration for Dmitry Galkovsky\",[\"Dmitry Galkovsky\"],[\"Rozanov\"]],\n",
        "                        [\"Jhon and Adam became inspiration for Mark\",[\"Mark\"],[\"John and Adam\"]],\n",
        "                        [\"Jhon's work provided inspiration for Mark\",[\"Mark\"],[\"John\"]],\n",
        "                        [\"He got the inspiration for this text from Schleiermacher ’ s Über die Religion \",[\"He\"],[\"Schleiermacher\"]],\n",
        "                        [\"Jhon's work was inspiration for Mark\",[\"Mark\"],[\"John\"]],\n",
        "                        [\"While Murdoch 's thought is an inspiration for Conradi\",[\"Conradi\"],[\"Murdoch\"]],\n",
        "                        [\"Jhon's work served as inspiration to Mark\",[\"Mark\"],[\"Jhon\"]],\n",
        "                        [\"Lucian 's True Story inspired Cyrano de Bergerac , whose writings later served as inspiration for Jules Verne \",[\"Cyrano de Bergerac\", \"Jules Verne\"],[\"Lucian\"]],\n",
        "                        [\"Mark took inspiration from John\",[\"Mark\"],[\"John\"]],\n",
        "                        [\"He also took inspiration from phenomenologist epistemology\",[\"He\"],[\"phenomenologist epistemology\"]],\n",
        "                        [\"Mark drew inspiration from John\", [\"Mark\"], [\"John\"]],\n",
        "                        [\"In particular , he drew inspiration from a Chinese Buddhist master named Tao-cho\",[\"he\"],[\"Tao-cho\"]],\n",
        "                        [\"Mark provided inspiration to John\",[\"John\"],[\"Mark\"]]\n",
        "                    ]\n",
        "phrases_inspired=[\n",
        "                    [\"He was inspired by him\",[\"He\"],[\"him\"]],\n",
        "                    [\"Mark have inspired Jhon\",[\"John\"],[\"Mark\"]],\n",
        "                    [\"Jhon had been inspired by Mark\",[\"John\"],[\"Mark\"]],\n",
        "                    [\"In it , Petrarch claimed to have been inspired by Philip V of Macedon\",[\"Petrarch\"],[\"Philip V of Macedon\"]],\n",
        "                    [\"Jhon's thinking was inspired by Mark\",[\"John\"],[\"Mark\"]],\n",
        "                    [\"Newton 's work on infinite series was inspired by Simon Stevin 's decimals\",[\"Newton\"],[\"Simon Stevin\"]],\n",
        "                    [\"Jhon's work was inspired by Marks\",[\"John\"],[\"Marks\"]],\n",
        "                    [ \"Schiller was inspired by the play Julius of Tarent by Johann Anton Leisewitz .\",[\"Schiller\"],[\"Johann Anton Leisewitz\"]],\n",
        "                    [ \"Mark was inspired by Jhon\",[\"Mark\"],[\"John\"]],\n",
        "                    [\"As a youth , he was inspired by Mencius ’ proposition\",[\"he\"],[\"Mencius\"]],\n",
        "                    [\"Jhon inspired Mark\",[\"John\"],[\"Mark\"]],\n",
        "                    [ \"It also inspired him to take falsifiability as his criterion of demarcation between what is , and is not , genuinely scientific\",[\"him\"],[\"It\"]],\n",
        "                    [\"Jhon inspired Mark's work\",[\"Mark\"],[\"John\"]],\n",
        "                    [ \"Spinoza inspired the poet Shelley to write his essay\",[\"Shelley\"],[\"Spinoza\"]]\n",
        "                  ]\n",
        "\n",
        "testset[\"phrases_influence\"] = phrases_influence\n",
        "testset[\"phrases_influenced\"] = phrases_influenced\n",
        "testset[\"phrases_influences\"] = phrases_influences\n",
        "testset[\"phrases_inspiration\"] = phrases_inspiration\n",
        "testset[\"phrases_influence\"] = phrases_influence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWFl--Pruu-j",
        "colab_type": "text"
      },
      "source": [
        "#***Import JSON articles***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDR8TdRiyuvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "{\n",
        "    \"philosopher\": \"name\",\n",
        "    \"article\": \"plaintext_article\",\n",
        "    \"pageid\": \"id\",\n",
        "    \"table_influenced\": \n",
        "        [\n",
        "            \"name_of_someone_philosopher_influenced_by\"\n",
        "        ]\n",
        "    \"table_influences\":\n",
        "        [\n",
        "\n",
        "            \"name_of_someone_philosopher_influences\"\n",
        "        ]\n",
        "}\n",
        "\"\"\"\n",
        "#open the file\n",
        "with open('/content/drive/My Drive/folder/result_1.json') as f:\n",
        "  jsonlist = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NU3jhlTPcgD",
        "colab_type": "text"
      },
      "source": [
        "# ***Text Preprocessing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4C1w8OESdrO",
        "colab_type": "code",
        "outputId": "d4857fc9-0c2e-46ff-a2a0-725e62a4984f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "\"\"\"\n",
        "#HIM\n",
        "#divide in sentences the article\n",
        "\n",
        "#eliminate sentences that don't include influenc -e, -ed, -es, -ing etc\n",
        "\n",
        "#pattern matching on patterns (see language analysis)\n",
        "\n",
        "#Entity recognition and recostruction\n",
        "\n",
        "#US\n",
        "\"mark influenced jhon. Today is friday\"\n",
        "#divide in sentences the article\n",
        "\"mark influenced jhon\"\n",
        "\"Today is friday\"\n",
        "#eliminate sentences that don't include influenc -e, -ed, -es, etc\n",
        "\"mark influenced jhon\"\n",
        "\n",
        "#pattern matching on patterns (using spacy \n",
        "?)\n",
        "[x = \"mark\", y = \"jhon\"]\n",
        "\n",
        "#Entity recognition and recostruction(here we have a proble, when he had Hendrix he could )\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for elem in jsonlist:\n",
        "    #divide in sentences the article\n",
        "    sent_list = nltk.sent_tokenize(elem[\"article\"])\n",
        "    \n",
        "    sent_list = [word_tokenize(i) for i in sent_list]\n",
        "\n",
        "    influence_list = []\n",
        "    influence_declinations = [\"influence\", \"influenced\", \"influences\", \"inspired\", \"inspiration\"]\n",
        "\n",
        "\n",
        "    for word_list in sent_list:\n",
        "        temp = [x for x in word_list if x in influence_declinations]\n",
        "        if len(temp) != 0:\n",
        "            influence_list.append(' '.join(word for word in word_list))\n",
        "\n",
        "    new_list = []\n",
        "    for sent in influence_list:\n",
        "        new_list.append(sent)\n",
        "        #displacy.render(sent, style=\"dep\", jupyter=True)\n",
        "    elem[\"article\"] = new_list\n",
        "print(time.time() - start)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f111f1fc1724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjsonlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#divide in sentences the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0msent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0msent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW3Mf9UlSeKx",
        "colab_type": "text"
      },
      "source": [
        "# ***Processing Giuliano(Spacy Dependencies)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rkKZ0aFSn-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2LAGk9n46KC",
        "colab_type": "text"
      },
      "source": [
        "# ***Testing Giuliano***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwU02BGMvYMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "------------------HIS APPROACH-------------------------\n",
        "1. X cite(s|d) Y1, Y2, …, Yn as (an) influence(s)\n",
        "2. X was influenced by Y1, Y2, …, Yn\n",
        "3. Y has been cited as an influence by X1, X2, …, Xn\n",
        "4. Y influence on X1, X2, …, Xn …\n",
        "5. Y1, Y2, …, Yn influenced hi(m|s) …\n",
        "\n",
        "\n",
        "------------------OUR APPROACH-------------------------\n",
        "Influence:\n",
        "X be an influence (of/on/for/to) Y \n",
        "X's influence (of/on/for/to) Y\n",
        "X's have influence on Y\n",
        "X be Y's influence\n",
        "Y cite X as influence\n",
        "X was cited as influence by Y\n",
        "\n",
        "\n",
        "influences:\n",
        "influences (include/of) X\n",
        "X be Y's influences\n",
        "X and X were influences of Y\n",
        "\n",
        "\n",
        "influencing:\n",
        "none\n",
        "\n",
        "\n",
        "influenced:\n",
        "Y be influenced by X\n",
        "X have influenced Y\n",
        "X influenced Y\n",
        "Y's thinking was influenced by X\n",
        "X influenced Y's work\n",
        "X's work influenced Y\n",
        "\n",
        "\n",
        "X ispiratore Y ispirante\n",
        "inspired\n",
        "Y be influenced by X\n",
        "X have inspired Y\n",
        "Y had been inspired by X\n",
        "Y's thinking/work was inspired by X\n",
        "Y was inspired by X\n",
        "X inspired Y\n",
        "\n",
        "\n",
        "inspire\n",
        "none\n",
        "\n",
        "\n",
        "inspiration\n",
        "X became/was inspiration for Y\n",
        "X's work provided/was inspiration for Y\n",
        "X's work served as inspiration to Y\n",
        "Y took/drew inspiration from X\n",
        "X provided inspiration to Y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Prints dependencies and displacy visuals for a phrase\n",
        "Params: sent, a sentence\n",
        "Returns: -\n",
        "\"\"\"\n",
        "def get_info(sent):\n",
        "    print(sent)\n",
        "    doc = nlp(sent)\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True)  \n",
        "    for token in doc:\n",
        "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "                [child for child in token.children])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Spacy processing\n",
        "Params: phrase, a phrase\n",
        "Returns:    influenced, list of influenced chunks\n",
        "            influencers, list of influencers chunks\n",
        "\"\"\"\n",
        "def process_spacy(phrase):\n",
        "\n",
        "    return [],[]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Process influence or inspiration phrase\n",
        "Params: phrase, a phrase tagged \"influence\" or \"inspiration\"\n",
        "Returns:    influenced, list of influenced chunks\n",
        "            influencers, list of influencers chunks\n",
        "\n",
        "\"\"\"\n",
        "#add climbing three for the subj\n",
        "def process_influence(phrase):\n",
        "    influenced = []\n",
        "    influencers = []\n",
        "    doc = nlp(phrase)\n",
        "    compounds = []\n",
        "    for token in doc:\n",
        "        #è un soggetto passivo, ex jhon is cited as an influence \n",
        "        if token.dep_ == \"nsubjpass\" and token.head.pos_ == \"VERB\" :\n",
        "            influencers.append(token.text)\n",
        "\n",
        "        #è un soggetto attivo, ex mark was an influence\n",
        "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"AUX\":\n",
        "            influencers.append(token.text)\n",
        "\n",
        "        #è un soggetto attivo, ex mark cited jhon as an influence\n",
        "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
        "            influenced.append(token.text)\n",
        "        \n",
        "        #è un oggetto\n",
        "        if token.dep_ in [\"pobj\", \"conj\", \"appos\", \"dobj\"]:\n",
        "            flag = 0\n",
        "            tok = token.text\n",
        "            while flag == 0:\n",
        "                if token.head.text == \"influence\":\n",
        "                    #see if it is passive or active\n",
        "                    if \"auxpass\" in [x.dep_ for x in token.head.children]:\n",
        "                        influencers.append(tok)\n",
        "                    else:\n",
        "                        influenced.append(tok)\n",
        "                    flag = 1\n",
        "                elif token.head.text == \"cited\" or token.head.text == \"cites\":\n",
        "                    if token.dep_ == \"agent\":\n",
        "                        influenced.append(tok)\n",
        "                    else:\n",
        "                        influencers.append(tok)\n",
        "\n",
        "                    flag = 1\n",
        "                elif token.head.dep_ in [\"prep\", \"agent\", \"conj\", \"appos\", \"dobj\", \"pobj\"]:\n",
        "                    token = token.head\n",
        "                else:\n",
        "                    flag = 1\n",
        "\n",
        "                \n",
        "                \n",
        "        #è un genitivo\n",
        "        if token.dep_ == \"poss\":\n",
        "            tok = token.text\n",
        "            token = token.head\n",
        "            flag = 0\n",
        "            while flag == 0:\n",
        "                if token.text == \"influence\":\n",
        "                    if token.dep_ in [\"attr\", \"pobj\", \"dobj\"]:\n",
        "                        influenced.append(tok)\n",
        "                    else:\n",
        "                        influencers.append(tok)\n",
        "                    flag = 1\n",
        "                elif token.head.dep_ in [\"prep\", \"agent\", \"conj\", \"appos\", \"dobj\", \"pobj\"]:\n",
        "                    token = token.head\n",
        "                else:\n",
        "                    flag = 1\n",
        "\n",
        "            \n",
        "                \n",
        "        #catch compounds\n",
        "        if token.dep_ == \"compound\":\n",
        "            compounds.append(token)\n",
        "\n",
        "    #look for compound names\n",
        "    for token in compounds:\n",
        "        if token.head.text in influenced:\n",
        "            influenced.append(token.text)\n",
        "        if token.head.text in influencers:\n",
        "            influencers.append(token.text)\n",
        "    return influenced, influencers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Process influenced or inspired phrase\n",
        "Params: phrase, a phrase tagged \"influencer\" or \"inspired\"\n",
        "Returns:    influenced, list of influenced chunks\n",
        "            influencers, list of influencers chunks\n",
        "\n",
        "\"\"\"\n",
        "#aggiungi compound e influenced\n",
        "#add climbing three for the subj\n",
        "def process_inspired(phrase):\n",
        "    influenced = []\n",
        "    influencers = []\n",
        "    doc = nlp(phrase)\n",
        "    compounds = []\n",
        "    for token in doc:\n",
        "        #è un soggetto passivo, ex Mark was infuenced\n",
        "        if token.dep_ == \"nsubjpass\" and token.head.text == \"inspired\" :\n",
        "            influenced.append(token.text)\n",
        "\n",
        "        #è un soggetto attivo, ex Jhon influenced\n",
        "        if token.dep_ == \"nsubj\" and token.head.text == \"inspired\":\n",
        "            influencers.append(token.text)\n",
        "\n",
        "            #aggiungi possibilità di un compound\"\"\"                                               aggiungi\n",
        "\n",
        "        #è un oggetto\n",
        "        if token.dep_ in [\"pobj\", \"conj\", \"appos\", \"dobj\"]:\n",
        "            flag = 0\n",
        "            tok = token.text\n",
        "            while flag == 0:\n",
        "                if token.head.dep_ in [\"prep\", \"agent\", \"conj\", \"appos\", \"dobj\", \"pobj\"]:\n",
        "                    token = token.head\n",
        "                elif token.head.text == \"inspired\":\n",
        "                    #see if it is passive or active\n",
        "                    if \"auxpass\" in [x.dep_ for x in token.head.children]:\n",
        "                        influencers.append(tok)\n",
        "                    else:\n",
        "                        influenced.append(tok)\n",
        "                    flag = 1\n",
        "                else:\n",
        "                    flag = 1\n",
        "        \n",
        "        #è un genitivo\n",
        "        if token.dep_ == \"poss\":\n",
        "            tok = token.text\n",
        "            token = token.head\n",
        "            #di un soggetto\n",
        "            if token.dep_== \"nsubjpass\" and token.head.text == \"inspired\" :\n",
        "                influenced.append(tok)\n",
        "            if token.dep_ == \"nsubj\" and token.head.text == \"inspired\":\n",
        "                influencers.append(tok)\n",
        "            #di un oggetto\n",
        "            if token.dep_ in [\"pobj\", \"conj\", \"appos\", \"dobj\"]:\n",
        "                flag = 0\n",
        "                while flag == 0:\n",
        "                    if token.head.dep_ in [\"prep\", \"agent\", \"conj\", \"appos\", \"dobj\", \"pobj\"]:\n",
        "                        token = token.head\n",
        "                    elif token.head.text == \"inspired\":\n",
        "                        #see if it is passive or active\n",
        "                        if \"auxpass\" in [x.dep_ for x in token.head.children]:\n",
        "                            influencers.append(tok)\n",
        "                        else:\n",
        "                            influenced.append(tok)\n",
        "                        flag = 1\n",
        "                    else:\n",
        "                        flag = 1\n",
        "\n",
        "\n",
        "        #catch compounds\n",
        "        if token.dep_ == \"compound\":\n",
        "            compounds.append(token)\n",
        "\n",
        "    #look for compound names\n",
        "    for token in compounds:\n",
        "        if token.head.text in influenced:\n",
        "            influenced.append(token.text)\n",
        "        if token.head.text in influencers:\n",
        "            influencers.append(token.text)\n",
        "    return influenced, influencers\n",
        "    \n",
        "\n",
        "\n",
        "def eval_processing(processing_method):\n",
        "    for phr in phrases_influence:\n",
        "        get_info([phr[0]])\n",
        "        influenced, influencer = process_influence(phr[0])\n",
        "        print(\"______EVAL______\")\n",
        "        print(\"influenced = \",phr[1], \"influencers\", phr[2])\n",
        "        print(\"influenced = \",influenced, \"influencers\", influencer)\n",
        "        print(len([x for x in influenced if x in phr[1]]), \"on\", len(phr[1]))\n",
        "        print(len([x for x in influencer if x in phr[2]]), \"on\", len(phr[2]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j132TVHEKK3Q",
        "colab_type": "text"
      },
      "source": [
        "# ***Processing Mattia(Regex)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrBeoeMOKPs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Input cella:\n",
        "jsonlist = \n",
        "[\n",
        "    {\n",
        "        \"philosopher\" = \"philosopher_name\",\n",
        "        \"pageid\" = id,\n",
        "        \"article\" = \n",
        "        [\n",
        "            \"frase contenente influence\",\n",
        "\n",
        "            \"frase contenente influence\", etc\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "Output cella:\n",
        "output =\n",
        "[\n",
        "    {\n",
        "        \"philosopher\" = \"philosopher_name\",\n",
        "        \"pageid\" = id,\n",
        "        \"article\" = \n",
        "        [\n",
        "            [\n",
        "                \"frase contenente influence\",\n",
        "                [\n",
        "                    list of influencers\n",
        "                ],\n",
        "                [\n",
        "                    list of influenced\n",
        "                ]\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#some prints and infos\n",
        "\n",
        "pprint.pprint(jsonlist)\n",
        "print(len(jsonlist))\n",
        "jsonlist_no_empty = [i for i in jsonlist if len(i[\"article\"]) != 0]\n",
        "print(len(jsonlist_no_empty))\n",
        "print([[i[\"philosopher\"], len(i[\"article\"]), i[\"article\"]] for i in jsonlist_no_empty if i[\"philosopher\"] == \"Socrates\"])\n",
        "\"\"\"\n",
        "#pprint.pprint(jsonlist)\n",
        "stringone = 'His philosophy is mainly influenced by such thinkers as Nietzsche , Epicurus , the Cynic and Cyrenaic schools , as well as French materialism .'\n",
        "relations = re.findall(r'[A-Z][a-z]+[\\s]', stringone)\n",
        "influence = re.findall(r'[iI]nfluenced[\\s]', stringone)\n",
        "dio = re.findall(r'[A-Z][a-z]+[\\s].*[iI]nfluenced[\\s].*[A-Za-z]+[\\s]', stringone)\n",
        "#print(re.split('[iI]nfluenced[\\s]by', stringone ,2)[1])\n",
        "left = re.split('[iI]nfluenced[\\s]by', stringone ,2)[0]\n",
        "#left = re.findall(r'[A-Z][a-z]+[\\s].*[iI]nfluenced[\\s]', stringone)\n",
        "right = re.split('[iI]nfluenced[\\s]by', stringone ,2)[1]\n",
        "#right = re.findall(r'[iI]nfluenced by[\\s].*', stringone)\n",
        "influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "\"\"\"\n",
        "print(stringone)\n",
        "print('Left: ' + str(left))\n",
        "print('Right: ' + str(right))\n",
        "print('Influencers: ' + str(influencers))\n",
        "print('Influenced: ' + str(influenced))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "jsonlist_no_empty = [i for i in jsonlist if len(i[\"article\"]) != 0]\n",
        "\n",
        "for pippo in jsonlist_no_empty:\n",
        "  #print(len(pippo['article']))\n",
        "  #print(pippo.keys())\n",
        "  pippo['influencers'] = []\n",
        "  pippo['influenced'] = []\n",
        "  pippo['rich_article'] = {}\n",
        "  for i in range(len(pippo['article'])):\n",
        "    left = []\n",
        "    right = []\n",
        "    influencers = []\n",
        "    influenced = []\n",
        "\n",
        "    #print('Sentence: ' + pippo['article'][i])\n",
        "    try:\n",
        "      left = re.split('have[\\s].*[iI]nfluenced[\\s]', pippo['article'][i] ,2)[0]\n",
        "      right = re.split('have[\\s].*[iI]nfluenced[\\s]', pippo['article'][i] ,2)[1]\n",
        "      influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "      influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "    except:\n",
        "      try:\n",
        "        left = re.split('was[\\s].*[iI]nfluenced[\\s]', pippo['article'][i] ,2)[0]\n",
        "        right = re.split('was[\\s].*[iI]nfluenced[\\s]', pippo['article'][i] ,2)[1]\n",
        "        influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "        influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "      except:\n",
        "        try:\n",
        "          left = re.split('[iI]nfluenced[\\s]by[\\s]', pippo['article'][i] ,2)[0]\n",
        "          right = re.split('[iI]nfluenced[\\s]by[\\s]', pippo['article'][i] ,2)[1]\n",
        "          influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "          influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "        except:\n",
        "          try:\n",
        "            left = re.split('has[\\s][iI]nfluenced[\\s]', pippo['article'][i] ,2)[0]\n",
        "            right = re.split('has[\\s][iI]nfluenced[\\s]', pippo['article'][i] ,2)[1]\n",
        "            influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "            influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "          except:\n",
        "            try:\n",
        "              left = re.split('[iI]nfluenced[\\s]', pippo['article'][i] ,2)[0]\n",
        "              right = re.split('[iI]nfluenced[\\s]', pippo['article'][i] ,2)[1]\n",
        "              influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "              influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "            except:\n",
        "              try:\n",
        "                left = re.split('[iI]nfluence[\\s].*on[\\s]', pippo['article'][i] ,2)[0]\n",
        "                right = re.split('[iI]nfluence[\\s].*on[\\s]', pippo['article'][i] ,2)[1]\n",
        "                influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "                influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "              except:\n",
        "                try:\n",
        "                  left = re.split('[iI]nfluence[\\s].*of[\\s]', pippo['article'][i] ,2)[0]\n",
        "                  right = re.split('[iI]nfluence[\\s].*of[\\s]', pippo['article'][i] ,2)[1]\n",
        "                  influencers = re.findall(r'[A-Z][a-z]+[\\s]', str(right))\n",
        "                  influenced = re.findall(r'[A-Z][a-z]+[\\s]', str(left))\n",
        "                except:\n",
        "                  influencers = []\n",
        "                  influenced = []\n",
        "    pippo['rich_article'][i] = {}\n",
        "    pippo['rich_article'][i]['phrase'] = pippo['article'][i]\n",
        "    pippo['rich_article'][i]['influenced'] = influenced\n",
        "    pippo['rich_article'][i]['influencers'] = influencers\n",
        "\n",
        "    pippo['influencers'].append(influencers)\n",
        "    pippo['influenced'].append(influenced)\n",
        "    #print('Influencers: ' + str(influencers))\n",
        "    #print('Influenced: ' + str(influenced))\n",
        "\n",
        "for pippo in jsonlist_no_empty:\n",
        "  pippo['influenced'] = [item for sublist in pippo['influenced'] for item in sublist]\n",
        "  pippo['influencers'] = [item for sublist in pippo['influencers'] for item in sublist]\n",
        "\n",
        "\n",
        "  pprint.pprint(pippo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbO5M4vrlJzN",
        "colab_type": "text"
      },
      "source": [
        "# ***Create Processed Json***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Ov7FTGlpD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ef7a65ef-3dbe-4393-9815-dffb8e68b2d5"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Input cella:\n",
        "jsonlist = \n",
        "[\n",
        "    {\n",
        "        \"philosopher\" = \"philosopher_name\",\n",
        "        \"pageid\" = id,\n",
        "        \"article\" = \n",
        "        [\n",
        "            \"frase contenente influence\",\n",
        "\n",
        "            \"frase contenente influence\", etc\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "Output cella:\n",
        "output =\n",
        "[\n",
        "    {\n",
        "        \"philosopher\" : \"philosopher_name\",\n",
        "        \"pageid\" : id,\n",
        "        \"article\" : \n",
        "            [\n",
        "                \"frase contenente influence\",\n",
        "\n",
        "                \"frase contenente influence\" \n",
        "            ]\n",
        "\n",
        "        \"rich_article\" :\n",
        "            [\n",
        "                {\n",
        "                    'influenced': ['East ', 'Asia '],\n",
        "                    'influencers': ['Influence ', 'Zhuangzi ',\"Plato\", \"William\", \"Ralph\", \"Inge\"],\n",
        "                    'phrase': '== Influence == Zhuangzi has influenced '\n",
        "                                'thinking far beyond East Asia .'\n",
        "                }\n",
        "            ]\n",
        "    }\n",
        "\n",
        "]\n",
        "\"\"\"\n",
        "#some prints and infos\n",
        "\n",
        "#pprint.pprint(jsonlist)\n",
        "print(len(jsonlist))\n",
        "jsonlist_no_empty = [i for i in jsonlist if len(i[\"article\"]) != 0]\n",
        "print(len(jsonlist_no_empty))\n",
        "#pprint.pprint(jsonlist_no_empty)\n",
        "\n",
        "jsonlist_processed = jsonlist_no_empty\n",
        "\n",
        "for phil in jsonlist_processed:\n",
        "    rich_article = []\n",
        "    for phrase in phil[\"article\"]:\n",
        "        #use regex for regex processing or process_spacy for spacy processing\n",
        "        influenced, influencers = process_influence(phrase)\n",
        "        elem = {}\n",
        "        elem[\"influenced\"] = influenced\n",
        "        elem[\"influencers\"] = influencers\n",
        "        elem[\"phrase\"] = phrase\n",
        "        rich_article.append(elem)\n",
        "    phil[\"rich_article\"] = rich_article\n",
        "\n",
        "pprint.pprint(jsonlist_processed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1712\n",
            "899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4LFfwUCmF9L",
        "colab_type": "text"
      },
      "source": [
        "# ***Cleaning and Recostruction(Luigi)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccUTAu2cmGcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "                \"frase contenente influence\" = \"Jhon Milton influenced Ghandi and Hannibal Bures' \"Power to the Man\"\"\n",
        "                [\n",
        "                    list of influencers = [\"Jhon\", \"Milton\"] --> \"Jhon Milton\"\n",
        "                ],\n",
        "                [\n",
        "                    list of influenced = [\"Ghandi\", \"Hannibal\", \"Bures\", \"Power\", \"Man\"] --> \"Mathma Ghandi\", \"Hannibal Bures\"\n",
        "                ]\n",
        "                \"him\" --> nome del philosopher\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}